# keravnos
Mixed-precision CUDA implementation of self-attention and projection layers, optimised for training Transformers on constrained memory hardware. Focused on tile-level performance, dropout masking, and low-overhead bias addition.
